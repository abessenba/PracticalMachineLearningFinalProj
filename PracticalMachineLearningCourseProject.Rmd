---
title: "Using Practical Machine Learning to Predict Human Activity Recognition"
author: "Ann Bessenbacher"
date: "July 1, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
The purpose of the Practical Machine Learning Course project is to predict the manner in which a group of 6 healthy subjects completed one set of 10 repititions of the Unilateral Dumbbell Biceps Curl in five different ways (A-E). The 'Classe' variable represents the five different ways to complete the exercise, with A being correct and B-E being alternate incorrect ways to complete the exercise.  More details on the data and the data itself can be found at the HAR website http://groupware.les.inf.puc-rio.br/har. Three different models will be tested below (decision trees, random forests and linear discriminate analysis).  The Random Forests model had the best fit model with a prediction accuracy of 100%.

## Data Loading
First we need to read in the libraries that will be needed to run the analysis, be sure to preload these packages if you want to recreate this analysis. We also need to set the seed to be sure we can recreate the predictions below. Then we read in the training and testing sets from files we previously downloaded from the above URL. You could read them in directly from the URL if you prefer.
```{r dataload, message=FALSE, warning=FALSE}
setwd("~/R/PracticalMachineLearning")
library(caret)
library(rpart)
library(MASS)
library(randomForest)
set.seed(8732)
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
```
## Data Cleaning and PreProcessing
Now that we have the data files needed, we can remove unnecessary variables before proceeding. As seen in the commented R code here:
```{r dataclean, message=FALSE, warning=FALSE}
## find columns with all NA's and remove them
colNas <- (colSums(is.na(training))==0)
training <- training[,colNas]
testing <- testing[,colNas]

## remove the first column that is a count variable and means nothing
## along with the timestamp columns 3-6 in both the training and test sets
training <- training[,-c(1,3:6)]
testing <- testing[,-c(1,3:6)]

## now we will use the nearZeroVar function to preprocess the data  
nsv <- nearZeroVar(training)
training <- training[,-nsv]
testing <- testing[,-nsv]
```

## Partitioning Training Set
Now that we have the training and testing data sets cleaned and preprocessed, we will partition the training set to create a validation set.
```{r partition, message=FALSE, warning=FALSE}
inTrain <- createDataPartition(y=training$classe, p=0.60, list=FALSE)
training <- training[inTrain,]
validation <- training[-inTrain,]

```
## Fitting models
In the code below we will fit four different models (Decision Tree, Random Forests, Linear Discriminate Analysis, and Boosted Trees) in order to find one with the best accuracy to predict the classe variable.  With each of the models we will use cross validation.  The plot of the Decision Tree can be found in the Appendix

```{r models, message=FALSE, warning=FALSE}
## Fit a Decision Tree model
modFitTree <- train(classe ~ ., method='rpart', trControl=trainControl(method='cv', number=3), data=training)

## Fit a Random Forests model
modFitRT <- randomForest(classe ~ ., trcontrol=trainControl(method='cv', number=3), data=training)

## Fit a Linear Discriminate Alanysis model
modFitLDA <- train(classe ~ ., method='lda', trcontrol=trainControl(method='cv', number=3), data=training)

```
## Model Assessment
In order to assess the four models we fitted above we need to predict the classe variable using the validation set we created and then run our prediction against the true value in the validation set using the confusion martrix.  We'll then place the model accuracy our measurement of out of sample error into a matrix. 
```{r modelassess, message=FALSE, warning=FALSE}
## Prediction using the Decision Tree model
predTrees <- predict(modFitTree, newdata=validation)
cmTrees <- confusionMatrix(predTrees, validation$classe)

## Prediction using the Random Forests model
predRF <- predict(modFitRT, newdata=validation)
cmRF <- confusionMatrix(predRF, validation$classe)

## Prediction using the Linear Discriminate Analysis model
predLDA <- predict(modFitLDA, newdata=validation)
cmLDA <- confusionMatrix(predLDA, validation$classe)

## Create Out of Sample Error/Accuracy Matrix from models
OSETable <- data.frame( Model = c('Decision Tree', 'Random Forests', 'Linear Discriminate Analysis'), Accuracy=rbind(cmTrees$overall[1], cmRF$overall[1], cmLDA$overall[1]))
print(OSETable)
```
## Final Prediction
As you can see above in the Accuracy matrix, the Random Forest model had 100% accuracy and was the best model. The Random Forest model accuracy table on the validation set can be found in the appendix below. Now we will use our cross validated Random Forest model to make our final prediction on the testing set that we have not touched so far except to remove columns like we did in the training set.  
```{r final, message=FALSE, warning=FALSE}
predFinal <- predict(modFitRT, newdata=testing)
print(predFinal)
```

## Appendix with Figures
```{r plots, message=FALSE, warning=FALSE}
## Decision tree
plot(modFitTree$finalModel, uniform=TRUE)
text(modFitTree$finalModel, use.n=TRUE, all=TRUE, cex=.8)

## Random Forest Accuracy on validation set
table(predRF, validation$classe)

```